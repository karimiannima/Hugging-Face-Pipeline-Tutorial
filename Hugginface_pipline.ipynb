{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6873c93-51b9-4c89-b9cd-b886d3cbbf40",
   "metadata": {},
   "source": [
    "# **Loading Models and Inference with Hugging Face Inferences with and without pipeline**\n",
    "\n",
    "We will explore the Hugging Face transformers library for NLP tasks. it start by manually implementing text classification and generation using models like DistilBERT and GPT-2, handling model loading, tokenization, inference, and output processing. Then, we will learn how the pipeline() function simplifies these tasks, achieving the same results with minimal code. By comparing both methods, we will see how pipeline() streamlines NLP implementation, saving time and effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bac06f-f972-4076-afff-ec3c88422d9d",
   "metadata": {},
   "source": [
    "# 1- Perform text classification and text generation using DistilBERT and GPT-2 models without pipeline()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90855c3c-ce31-4bbe-ac2d-99e0f47f1bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\sarat\\appdata\\roaming\\python\\python313\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\sarat\\appdata\\roaming\\python\\python313\\site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\sarat\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\sarat\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sarat\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ff8f7a-5e26-4436-9776-2fe4fc8942fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89497003-ccbf-44ff-917d-f295aa688c7c",
   "metadata": {},
   "source": [
    "# Text classification with DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebff8f-688e-4234-998d-c74545d53c90",
   "metadata": {},
   "source": [
    "## Load the model and tokenizer\n",
    "\n",
    "Let's start by initializing a DistilBERT tokenizer and model fine-tuned on the SST-2 dataset for sentiment analysis. This setup enables efficient sentiment classification of text using a pretrained transformer model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dbc5720-42c3-4ed0-8c8d-d1afb605abf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd2b92cfc7e4da78b60ef1a45315911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d2de442da844c284ca677a8e0accbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4a85fd4f5a498bb74460a517d1e684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd9b898b6d4427c8023d19668c0fca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a34265-1f10-47d5-8d03-2afb213fdf47",
   "metadata": {},
   "source": [
    "#  Preprocess the Input Text\n",
    "This code takes the input text, tokenizes it, and converts it into a PyTorch tensor format (\"pt\"), which is suitable for the model. Make sure to replace text with your actual input string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f8e9833-5960-4f6b-b33e-8a106a7baf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  1049,  2061,  7568,  2005,  2026,  9046, 10885,\n",
      "          2000,  7359,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "# Define the text\n",
    "#text = \"Your text here\"\n",
    "# example\n",
    "text = \"I'm so excited for my upcoming vacation to Hawaii!\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56b71f-334e-42ff-9ac3-9886dbbd7865",
   "metadata": {},
   "source": [
    "The token IDs represent the token indexes, while the attention_mask helps the model distinguish between actual content and padding, ensuring efficient computation and accurate processing of input data, even when no tokens are explicitly masked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae0809-c129-4ca5-ab17-4706e1f95d6c",
   "metadata": {},
   "source": [
    "###  Perform inference\n",
    "To run inference, we'll use the  `torch.no_grad()` context manager to disable gradient calculation, reducing memory usage and speeding up computation since gradients aren't needed when the model isn't being trained.\n",
    "The **inputs syntax unpacks the dictionary of keyword arguments, allowing us to pass the tokenized inputs directly to the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcbe2540-4b05-4f98-886a-70dd2f66cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee62dc-e8e9-422d-8728-6e3b535ebb32",
   "metadata": {},
   "source": [
    "Another method is `input_ids`, and `attention_mask` is their own parameter. Instead of using **inputs to unpack the dictionary, you can pass the input_ids and attention_mask as separate parameters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3bbb71-00da-4744-86c6-bedad3880aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    "#    logits = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00709a41-9e4b-455e-986c-7dce674c39c9",
   "metadata": {},
   "source": [
    "#### Get the logits\n",
    "The logits represent the model's raw, unnormalized predictions. We can extract these logits from the model's output for further processing, like determining the predicted class or calculating probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f700f9bb-73c6-42f9-908e-7544edcc2301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f95fab-92bc-482e-84f3-1f8173e42fe5",
   "metadata": {},
   "source": [
    "## Post-process the output\n",
    "Convert the logits to probabilities and get the predicted class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "502f249f-65c7-4c2d-8f36-70fbf7eb2aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(probs, dim=-1)\n",
    "\n",
    "# Map the predicted class to the label\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "predicted_label = labels[predicted_class]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db3bcb-20d4-4e8b-920a-32db437fcac4",
   "metadata": {},
   "source": [
    "# Text generation with GPT-2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb5266-2152-4f0f-91fb-bb0db37049b5",
   "metadata": {},
   "source": [
    "## Load tokenizer\n",
    "Load the pre-trained GPT-2 tokenizer, which converts text into tokens that the model can process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fddfa80-364c-4927-8840-612a56f9c94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18eb8ece3cb4840b85ed873a0eae394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cad47ec3a24fef99c617cfd5809358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83609150f79425bb5ecacfbd102ead3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d3d50905a54b34b6561401c1de7619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bce661f6ee47a99b19f59a193822d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a17872-707e-4037-9cad-209a418dee34",
   "metadata": {},
   "source": [
    "Load the pre-trained GPT-2 model with a language modeling head, which generates text based on input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "716e4226-d1f7-41e8-977b-ff2276204134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28c54b370064835a4d71469baac4fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd7e416c86d410696339d3ca16901f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73257635-a56e-4510-aa28-c991062572f0",
   "metadata": {},
   "source": [
    "## Preprocess the input text  \n",
    "Tokenize the input text and convert it into a model-friendly format, resulting in token indexes or input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11798657-2067-4d21-b721-6cb3c8b56278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  40, 1842, 1152,  876, 9552, 1398]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = \"I love generative AI class\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e4227-3982-409d-9355-b4f1741091b1",
   "metadata": {},
   "source": [
    "## Perform inference  \n",
    "Generate text using the model with temperature sampling\n",
    "\n",
    "- **inputs:** Input token IDs from the tokenizer  \n",
    "- **attention_mask:** Mask indicating which tokens to attend to  \n",
    "- **pad_token_id:** Padding token ID set to the end-of-sequence token ID  \n",
    "- **max_length:** Maximum length of the generated sequences  \n",
    "- **num_return_sequences:** Number of sequences to generate  \n",
    "- **do_sample:** Enables sampling instead of greedy decoding  \n",
    "- **temperature:** Controls randomness in generation (lower = more focused, higher = more random)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b60fb-0786-4c5f-a2ea-7ef85949e026",
   "metadata": {},
   "source": [
    "### Notes on Sampling Parameters\n",
    "\n",
    "- **temperature = 1.0** → baseline (no scaling)  \n",
    "- **temperature < 1.0** → more focused / less random  \n",
    "  - Example: `0.7` makes outputs safer and more deterministic  \n",
    "- **temperature > 1.0** → more diverse / random  \n",
    "  - Example: `1.2 – 1.5` encourages creativity but may reduce coherence  \n",
    "\n",
    "You can also combine with:  \n",
    "- **top_k = 50** → sample only from the top 50 tokens  \n",
    "- **top_p = 0.9** → nucleus sampling (tokens making up 90% probability mass)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6623a0e-9646-438f-b76a-d00bee3c2f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,  1842,  1152,   876,  9552,  1398,  6637,    11,   290,   314,\n",
       "          1101,  1016,   284,   307, 11065,   428,   757,    11,   523,   314,\n",
       "          1101,  2111,   284,  1394,   428,  7243,   287,  2000,    13,   198,\n",
       "           198,    40,   716,  3058,  1762,   319,   257,   649,  1398,  7483,\n",
       "           329,   257,  1398,  7483,   329,   617,   286,   262,  1152,   876]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    attention_mask=inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50, \n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,        # enable sampling\n",
    "    temperature=0.7        # controls randomness (lower = more deterministic, higher = more random)\n",
    ")\n",
    "\n",
    "output_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0ebb4-cf6e-4204-98e1-277b45a0eeeb",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6bfc5-c16b-4ebe-a842-0a5a2473f09c",
   "metadata": {},
   "source": [
    "```python\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs) \n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c5707-bc12-4b04-92c6-d4ed31d1a24b",
   "metadata": {},
   "source": [
    "## Post-process the output  \n",
    "Decode the generated tokens to get the text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd436fe9-7ba3-4af1-b6e6-078925765cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love generative AI classifications, and I'm going to be studying this again, so I'm trying to keep this topic in mind.\n",
      "\n",
      "I am currently working on a new classifier for a classifier for some of the generative\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1038110-bf92-47fb-b33d-795ef1b813c5",
   "metadata": {},
   "source": [
    "# 2- Perform text classification and text generation using DistilBERT and GPT-2 models with pipeline()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a666d3a-70d9-41be-a3ff-bdf58c82b932",
   "metadata": {},
   "source": [
    "# Hugging Face `pipeline()` function\n",
    "\n",
    "The `pipeline()` function from the Hugging Face `transformers` library is a high-level API designed to simplify the usage of pretrained models for various natural language processing (NLP) tasks. It abstracts the complexities of model loading, tokenization, inference, and post-processing, allowing users to perform complex NLP tasks with just a few lines of code.\n",
    "\n",
    "## Definition\n",
    "\n",
    "```python\n",
    "transformers.pipeline(\n",
    "    task: str,\n",
    "    model: Optional = None,\n",
    "    config: Optional = None,\n",
    "    tokenizer: Optional = None,\n",
    "    feature_extractor: Optional = None,\n",
    "    framework: Optional = None,\n",
    "    revision: str = 'main',\n",
    "    use_fast: bool = True,\n",
    "    model_kwargs: Dict[str, Any] = None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- **task**: `str`\n",
    "  - The task to perform, such as \"text-classification\", \"text-generation\", \"question-answering\", etc.\n",
    "  - Example: `\"text-classification\"`\n",
    "\n",
    "- **model**: `Optional`\n",
    "  - The model to use. This can be a string (model identifier from Hugging Face model hub), a path to a directory containing model files, or a pre-loaded model instance.\n",
    "  - Example: `\"distilbert-base-uncased-finetuned-sst-2-english\"`\n",
    "\n",
    "- **config**: `Optional`\n",
    "  - The configuration to use. This can be a string, a path to a directory, or a pre-loaded config object.\n",
    "  - Example: `{\"output_attentions\": True}`\n",
    "\n",
    "- **tokenizer**: `Optional`\n",
    "  - The tokenizer to use. This can be a string, a path to a directory, or a pre-loaded tokenizer instance.\n",
    "  - Example: `\"bert-base-uncased\"`\n",
    "\n",
    "- **feature_extractor**: `Optional`\n",
    "  - The feature extractor to use for tasks that require it (e.g., image processing).\n",
    "  - Example: `\"facebook/detectron2\"`\n",
    "\n",
    "- **framework**: `Optional`\n",
    "  - The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. If not specified, it will be inferred.\n",
    "  - Example: `\"pt\"`\n",
    "\n",
    "- **revision**: `str`, default `'main'`\n",
    "  - The specific model version to use (branch, tag, or commit hash).\n",
    "  - Example: `\"v1.0\"`\n",
    "\n",
    "- **use_fast**: `bool`, default `True`\n",
    "  - Whether to use the fast version of the tokenizer if available.\n",
    "  - Example: `True`\n",
    "\n",
    "- **model_kwargs**: `Dict[str, Any]`, default `None`\n",
    "  - Additional keyword arguments passed to the model during initialization.\n",
    "  - Example: `{\"output_hidden_states\": True}`\n",
    "\n",
    "- **kwargs**: `Any`\n",
    "  - Additional keyword arguments passed to the pipeline components.\n",
    "\n",
    "## Task types\n",
    "\n",
    "The `pipeline()` function supports a wide range of NLP tasks. Here are some of the common tasks:\n",
    "\n",
    "1. **Text Classification**: `text-classification`\n",
    "   - **Purpose**: Classify text into predefined categories.\n",
    "   - **Use Cases**: Sentiment analysis, spam detection, topic classification.\n",
    "\n",
    "2. **Text Generation**: `text-generation`\n",
    "   - **Purpose**: Generate coherent text based on a given prompt.\n",
    "   - **Use Cases**: Creative writing, dialogue generation, story completion.\n",
    "\n",
    "3. **Question Answering**: `question-answering`\n",
    "   - **Purpose**: Answer questions based on a given context.\n",
    "   - **Use Cases**: Building Q&A systems, information retrieval from documents.\n",
    "\n",
    "4. **Named Entity Recognition (NER)**: `ner` (or `token-classification`)\n",
    "   - **Purpose**: Identify and classify named entities (like people, organizations, locations) in text.\n",
    "   - **Use Cases**: Extracting structured information from unstructured text.\n",
    "\n",
    "5. **Summarization**: `summarization`\n",
    "   - **Purpose**: Summarize long pieces of text into shorter, coherent summaries.\n",
    "   - **Use Cases**: Document summarization, news summarization.\n",
    "\n",
    "6. **Translation**: `translation_xx_to_yy` (e.g., `translation_en_to_fr`)\n",
    "   - **Purpose**: Translate text from one language to another.\n",
    "   - **Use Cases**: Language translation, multilingual applications.\n",
    "\n",
    "7. **Fill-Mask**: `fill-mask`\n",
    "   - **Purpose**: Predict masked words in a sentence (useful for masked language modeling).\n",
    "   - **Use Cases**: Language modeling tasks, understanding model predictions.\n",
    "\n",
    "8. **Zero-Shot Classification**: `zero-shot-classification`\n",
    "   - **Purpose**: Classify text into categories without needing training data for those categories.\n",
    "   - **Use Cases**: Flexible and adaptable classification tasks.\n",
    "\n",
    "9. **Feature Extraction**: `feature-extraction`\n",
    "   - **Purpose**: Extract hidden state features from text.\n",
    "   - **Use Cases**: Downstream tasks requiring text representations, such as clustering, similarity, or further custom model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327aeed-268e-4649-8bd2-c42344399bbd",
   "metadata": {},
   "source": [
    "### Example 1: Text classification using `pipeline()`\n",
    "\n",
    "This example demonstrates text classification using the  `pipeline()` function. We'll load a pre-trained model and classify a sample text.\n",
    "\n",
    "#### Load the model:\n",
    "Initialize the pipeline for text-classification with the \"distilbert-base-uncased-finetuned-sst-2-english\" model, fine-tuned for sentiment analysis.\n",
    "\n",
    "#### Classify the sample text:\n",
    "Use the classifier to evaluate the text: \"I'm so excited for my upcoming vacation to Hawaii!\" The classifier returns and prints the classification result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f569d168-e129-4deb-8c5a-81d46c944a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997586607933044}]\n"
     ]
    }
   ],
   "source": [
    "# Load a general text classification model\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Classify a sample text\n",
    "result = classifier(\"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b2d3a-0c66-4b2d-a560-3b63d425e150",
   "metadata": {},
   "source": [
    "#### Output\n",
    "\n",
    "The output will be a list of dictionaries, where each dictionary contains:\n",
    "\n",
    "- `label`: The predicted label (e.g., \"POSITIVE\" or \"NEGATIVE\").\n",
    "- `score`: The confidence score for the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29271acd-8f85-4a7d-a126-1b6dcd542dd1",
   "metadata": {},
   "source": [
    "### Example 2: Language detection using `pipeline()`\n",
    "\n",
    "In this example, you will use the `pipeline()` function to perform language detection. You will load a pretrained language detection model and use it to identify the language of a sample text.\n",
    "\n",
    "#### Load the language detection model:\n",
    "We initialize the pipeline for the `text-classification` task, specifying the model `\"papluca/xlm-roberta-base-language-detection\"`. This model is fine-tuned for language detection.\n",
    "\n",
    "#### Classify the sample text:\n",
    "We use the classifier to detect the language of a sample text: \"من خیلی برای تعطیلات آینده‌ام در هاوایی هیجان‌زده‌ام!\" The `classifier` function returns the classification result, which is then printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b04a98cd-a72c-4bf4-af52-7c9098c5f42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca90a5956610454d964ba6738b58333d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de475a2dee474a0a817c1549c47185e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362152ba12444de0bf0ecd18e332b78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d275607e36c245a7a6b3729f39831cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30610f48039843d7a82c943fb51b61f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f1102b9f6c4d3194e48aa940039ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'ur', 'score': 0.18315279483795166}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "result = classifier(\"من خیلی برای تعطیلات آینده‌ام در هاوایی هیجان‌زده‌ام!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88b991fa-a1e7-4911-812a-2541dbd66fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_30', 'score': 0.9999822378158569}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"Mike0307/multilingual-e5-language-detection\")\n",
    "result = classifier(\"من خیلی برای تعطیلات آینده‌ام در هاوایی هیجان‌زده‌ام!\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21ba955e-a304-4f4b-9fa9-6d69b59cf2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: Persian (id=30), score=1.0000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "model_name = \"Mike0307/multilingual-e5-language-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Languages listed in the model card, in order (index == label id)\n",
    "languages = [\n",
    "    \"Arabic\",\"Basque\",\"Breton\",\"Catalan\",\"Chinese_China\",\"Chinese_Hongkong\",\n",
    "    \"Chinese_Taiwan\",\"Chuvash\",\"Czech\",\"Dhivehi\",\"Dutch\",\"English\",\n",
    "    \"Esperanto\",\"Estonian\",\"French\",\"Frisian\",\"Georgian\",\"German\",\"Greek\",\n",
    "    \"Hakha_Chin\",\"Indonesian\",\"Interlingua\",\"Italian\",\"Japanese\",\"Kabyle\",\n",
    "    \"Kinyarwanda\",\"Kyrgyz\",\"Latvian\",\"Maltese\",\"Mongolian\",\"Persian\",\"Polish\",\n",
    "    \"Portuguese\",\"Romanian\",\"Romansh_Sursilvan\",\"Russian\",\"Sakha\",\"Slovenian\",\n",
    "    \"Spanish\",\"Swedish\",\"Tamil\",\"Tatar\",\"Turkish\",\"Ukranian\",\"Welsh\"\n",
    "]\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "text = \"من خیلی برای تعطیلات آینده‌ام در هاوایی هیجان‌زده‌ام!\"\n",
    "out = clf(text)[0]               # e.g. {'label': 'LABEL_30', 'score': 0.9999}\n",
    "idx = int(out[\"label\"].split(\"_\")[1])\n",
    "lang = languages[idx]\n",
    "\n",
    "print(f\"Detected language: {lang} (id={idx}), score={out['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf495a-95b9-475c-94cd-3ca70afe71ee",
   "metadata": {},
   "source": [
    "### Example 3: Text generation with `pipeline()`\n",
    "\n",
    "This example uses the `pipeline()`function for text generation. We'll load a pre-trained model and generate text based on a prompt.\n",
    "#### Initialize the text generation model:\n",
    "Initialize the pipeline for  `text-generation` with the `\"gpt2\"` model, a popular choice for text generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b843942-787c-43ed-9a7c-af74d2217db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline with GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081339f-1c5c-4330-8262-ec8b24595626",
   "metadata": {},
   "source": [
    "#### Generate text based on a given prompt:\n",
    "Use the generator with the prompt \"Once upon a time\" and specify parameters:\n",
    "`max_length=50` to limit the output to 50 tokens\n",
    "`truncation=True` for text truncation\n",
    "`num_return_sequences=1` to generate one sequence\n",
    "The generator returns and prints the generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0a502c5-0406-4c6f-84a4-e734f10bd859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love generative AI classifiers, and I also love the fact that it's easy to use and maintain, though I've never used it in my own codebase.\n",
      "\n",
      "Using generative AI, it's easy to run a classifier from a custom generated class, and it's easy to pick a class based on data, like a value of type int, and run a class based on a value of type type int.\n",
      "\n",
      "class C ( classname int ) { static int c = 0 ; } class D ( classname int ) { static int c = 0 ; } class E ( classname int ) { static int c = 0 ; } function main ( ) { //... }\n",
      "\n",
      "This is where generative AI really comes into its own. It's not just that C is able to run a classifier, but it can also run classifiers from a custom generated class. It's easier to use, and less to worry about.\n",
      "\n",
      "The main advantage of generative AI is that it can use just as many parameters as you want, instead of trying to find the best way to fit it. It also makes it easy to write your own classifiers, which is usually a good thing.\n",
      "\n",
      "class C ( classname int ) { static int c\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on a given prompt\n",
    "prompt = \"I love generative AI class\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828cf1b-94ab-4c47-af80-04720e5c90dc",
   "metadata": {},
   "source": [
    "### Example 4: Text-to-Text Generation with T5 and pipeline() `pipeline()`\n",
    "\n",
    "This example uses the `pipeline()`  function for text-to-text generation with the T5 model. We'll load a pre-trained T5 model and translate a sentence from English to French based on a prompt. \n",
    "\n",
    "#### Load the Model:\n",
    "Initialize the pipeline for text2text-generation with the \"t5-small\" model, a versatile model capable of various text-to-text tasks, including translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2eb6482-1f27-4e2e-8865-f41ebde09a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9b0d11a1954963b488348baac754e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8012ed47d2441a68983bef7ea4bfaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c239abfa2a884df9a7ff69621e5f1f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e625cc94144b359f6ca5f61480d621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e359127670148a6ae1413625a2db09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6de7c1a06d42fdac03b986c8b2e47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline with T5\n",
    "generator = pipeline(\"text2text-generation\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46efe9df-8632-4390-9b7a-39cca7cb88db",
   "metadata": {},
   "source": [
    "#### Generate Translation:\n",
    "Use the generator to translate \"How are you?\" from English to French with the prompt \"translate English to French: How are you?\". Specify:\n",
    "`max_length=50` to limit output to 50 tokens\n",
    "`num_return_sequences=1` for a single translation\n",
    "The generator returns and prints the translated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5c7aa0e-ce42-46cb-b23d-f63d481ceb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment êtes-vous?\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on a given prompt\n",
    "prompt = \"translate English to French: How are you?\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb998c9e-5818-4d7d-9841-24921574a14a",
   "metadata": {},
   "source": [
    "# Benefits of pipeline()\n",
    "Less Code: Reduces boilerplate code for NLP tasks.\n",
    "Readability: Improves code readability and expressiveness.\n",
    "Time-Saving: Automates model loading, tokenization, inference, and post-processing.\n",
    "Consistent API: Enables easy experimentation and prototyping with a unified API.\n",
    "Framework Flexibility: Handles underlying frameworks (TensorFlow or PyTorch) automatically.\n",
    "# Use pipeline() for:\n",
    "Rapid Prototyping: Quickly test NLP applications or models.\n",
    "Simple Tasks: Common NLP tasks well-supported by pipeline().\n",
    "Deployment: Environments requiring simplicity and ease of use.\n",
    "# Avoid pipeline() for:\n",
    "Custom Tasks: Tasks requiring high customization not supported by pipeline().\n",
    "Performance Tuning: Cases needing fine-grained control over models and tokenization for optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb855c85-90e9-4f2a-97db-7356a5e78fe3",
   "metadata": {},
   "source": [
    "# Fill-mask task with BERT with `pipeline()`\n",
    "\n",
    "Use the `pipeline()` function to perform a fill-mask task with the BERT model. Load a pre-trained BERT model to predict the masked word in a given sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c48a5-5210-4ac2-a472-64f481175cca",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. **Initialize the fill-mask pipeline** with the BERT model.\n",
    "2. **Create a prompt** with a masked token.\n",
    "3. **Generate text** by filling in the masked token.\n",
    "4. **Print the generated text** with the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10ea8ab4-45c5-4eb8-8cf1-c2a83f39ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    'tehran'  score=0.9968  -> the capital of the iran is tehran.\n",
      "      'iran'  score=0.0015  -> the capital of the iran is iran.\n",
      "   'yerevan'  score=0.0006  -> the capital of the iran is yerevan.\n",
      "    'kerman'  score=0.0005  -> the capital of the iran is kerman.\n",
      "      'baku'  score=0.0001  -> the capital of the iran is baku.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Whole-word masking BERT tends to give nicer fills\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-large-uncased-whole-word-masking\")\n",
    "\n",
    "mask = fill_mask.tokenizer.mask_token  # \"[MASK]\" for BERT, \"<mask>\" for RoBERTa\n",
    "#prompt = f\"The capital of the United States of America is {mask}.\"\n",
    "prompt = f\"The capital of the Iran is {mask}.\"\n",
    "preds = fill_mask(prompt, top_k=5)\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']!r:>12}  score={p['score']:.4f}  -> {p['sequence']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49e92d-b899-483b-be2e-b2323ec2585c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
